max_prompt_chars: 4000
block_on_match: true
log_samples: true

# Preprocessor configuration
vectorizer:
  model: "sentence-transformers/all-MiniLM-L6-v2"

# Qdrant configuration
qdrant:
  url: "http://localhost:6333"
  collection_name: "firewall_vectors"
  enabled: false

# ML models configuration
ml:
  pii_model: "models/pii_model.onnx"  # Path relativo desde /app en el contenedor
  toxicity_model: "models/toxicity_model.onnx"
  # Opcional: paths para tokenizers
  toxicity_tokenizer: "unitary/toxic-bert"  # O path local
  toxicity_detector_type: "detoxify"  # Change to 'detoxify' or 'onnx'
  detoxify_model_name: "original"  # 'original', 'unbiased', o 'multilingual'
  
  # Embeddings configuration for prompt injection detection
  # Local embeddings are ~50-200ms vs Ollama's ~2-5s
  use_local_embeddings: true  # Set to false to use Ollama API instead
  local_embedding_model: "nomic-ai/nomic-embed-text-v1.5"  # Compatible with Ollama's nomic model

# Heuristic detector configuration
heuristic:
  rules_path: "rules/prompt_injection_rules.yaml"

# Policy engine configuration
policy:
  policies_path: "policy_engine/policies.yaml"

# Logging configuration
logging:
  type: "print"